{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendas = pd.read_csv('../../../data/interim/Vendas.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendas = vendas.replace(['ST', 'CORSÁRIO SUPLEX', 'ESP', 'ESPECIAL'], ['00','CORSARIO SUPLEX', 'GG', 'GG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendas['DATE_BLOCK'] = LabelEncoder().fit_transform(vendas.ANO.apply(str) + vendas.MES.apply(lambda x: str(x).zfill(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer as vendas mensais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vendas.groupby(['ANO', 'MES', 'DESCRICAO', 'NOME', 'TAMANHO', 'DATE_BLOCK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = a.sum().reset_index().drop(['DIA', 'VLR_UNIT'], 1)\n",
    "df = df.sort_values(by=['ANO','MES', 'DESCRICAO', 'TAMANHO', 'NOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_year = np.array([])\n",
    "for i in np.unique(df.DATE_BLOCK):\n",
    "    media = list(filter(lambda x: x>=0, [j for j in range(i-1, i-13, -1)]))\n",
    "    df2 = df[df.DATE_BLOCK.isin(media)]\n",
    "    df2 = df2.groupby(['DESCRICAO','NOME', 'TAMANHO']).sum().reset_index()\n",
    "    a = df[df.DATE_BLOCK == i].merge(df2[['NOME', 'DESCRICAO', 'TAMANHO', 'QUANTIDADE']], on=['NOME', 'DESCRICAO', 'TAMANHO'], how='left')\n",
    "    last_year = np.append(last_year, (a.QUANTIDADE_y).fillna(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_year'] = last_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino = df[df.ANO < 2018]\n",
    "validacao = df[df.ANO >= 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANO</th>\n",
       "      <th>MES</th>\n",
       "      <th>DESCRICAO</th>\n",
       "      <th>NOME</th>\n",
       "      <th>TAMANHO</th>\n",
       "      <th>DATE_BLOCK</th>\n",
       "      <th>QUANTIDADE</th>\n",
       "      <th>last_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>ITAIM</td>\n",
       "      <td>00</td>\n",
       "      <td>23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4392</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>02</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4393</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>06</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>08</td>\n",
       "      <td>23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ANO   MES   DESCRICAO          NOME TAMANHO  DATE_BLOCK  QUANTIDADE  \\\n",
       "4391  2017.0  12.0  SHORT SAIA         ITAIM      00          23         2.0   \n",
       "4392  2017.0  12.0  SHORT SAIA  VERBO DIVINO      02          23         1.0   \n",
       "4393  2017.0  12.0  SHORT SAIA  VERBO DIVINO      06          23         3.0   \n",
       "4394  2017.0  12.0  SHORT SAIA  VERBO DIVINO      08          23         2.0   \n",
       "4395  2017.0  12.0  SHORT SAIA  VERBO DIVINO      10          23         1.0   \n",
       "\n",
       "      last_year  \n",
       "4391        4.0  \n",
       "4392       26.0  \n",
       "4393       91.0  \n",
       "4394       43.0  \n",
       "4395       31.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treino.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_DESC = LabelEncoder()\n",
    "treino['DESCRICAO'] = le_DESC.fit_transform(treino.DESCRICAO)\n",
    "validacao['DESCRICAO'] = le_DESC.transform(validacao.DESCRICAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_TAM = LabelEncoder()\n",
    "treino['TAMANHO'] = le_TAM.fit_transform(treino.TAMANHO)\n",
    "validacao['TAMANHO'] = le_TAM.transform(validacao.TAMANHO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_NOME = LabelEncoder()\n",
    "treino['NOME'] = le_NOME.fit_transform(treino.NOME)\n",
    "validacao['NOME'] = le_NOME.transform(validacao.NOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = treino['QUANTIDADE']\n",
    "y_val = validacao['QUANTIDADE']\n",
    "\n",
    "X_train = treino.drop(['QUANTIDADE', 'DATE_BLOCK'], 1)\n",
    "X_val = validacao.drop(['QUANTIDADE', 'DATE_BLOCK'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = OneHotEncoder(categorical_features=[1,2, 3, 4])\n",
    "X_train_ = one.fit_transform(X_train).toarray()\n",
    "X_val_ = one.transform(X_val).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treino[treino.ANO == 2017]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_ = sc.fit_transform(X_train)\n",
    "X_val_ = sc.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(59, input_dim=X_train_.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(5, input_dim=X_train_.shape[1], kernel_initializer='normal', activation='elu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "calbacks = [ModelCheckpoint(filepath='./model', monitor='val_loss', save_best_only=True, verbose=1), EarlyStopping(patience=20)]\n",
    "model = ret_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4396 samples, validate on 985 samples\n",
      "Epoch 1/1000\n",
      "4396/4396 [==============================] - 1s 172us/step - loss: 152.5520 - val_loss: 192.6982\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 192.69818, saving model to ./model\n",
      "Epoch 2/1000\n",
      "4396/4396 [==============================] - 0s 65us/step - loss: 113.1788 - val_loss: 144.8492\n",
      "\n",
      "Epoch 00002: val_loss improved from 192.69818 to 144.84921, saving model to ./model\n",
      "Epoch 3/1000\n",
      "4396/4396 [==============================] - 0s 55us/step - loss: 101.9183 - val_loss: 128.8539\n",
      "\n",
      "Epoch 00003: val_loss improved from 144.84921 to 128.85393, saving model to ./model\n",
      "Epoch 4/1000\n",
      "4396/4396 [==============================] - 0s 59us/step - loss: 98.8464 - val_loss: 124.5490\n",
      "\n",
      "Epoch 00004: val_loss improved from 128.85393 to 124.54901, saving model to ./model\n",
      "Epoch 5/1000\n",
      "4396/4396 [==============================] - 0s 64us/step - loss: 96.7371 - val_loss: 115.9767\n",
      "\n",
      "Epoch 00005: val_loss improved from 124.54901 to 115.97672, saving model to ./model\n",
      "Epoch 6/1000\n",
      "4396/4396 [==============================] - 0s 71us/step - loss: 95.9272 - val_loss: 109.1477\n",
      "\n",
      "Epoch 00006: val_loss improved from 115.97672 to 109.14774, saving model to ./model\n",
      "Epoch 7/1000\n",
      "4396/4396 [==============================] - 0s 74us/step - loss: 94.4725 - val_loss: 114.7910\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 109.14774\n",
      "Epoch 8/1000\n",
      "4396/4396 [==============================] - 0s 72us/step - loss: 93.5626 - val_loss: 109.7715\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 109.14774\n",
      "Epoch 9/1000\n",
      "4396/4396 [==============================] - 0s 69us/step - loss: 92.5591 - val_loss: 107.7290\n",
      "\n",
      "Epoch 00009: val_loss improved from 109.14774 to 107.72899, saving model to ./model\n",
      "Epoch 10/1000\n",
      "4396/4396 [==============================] - 0s 83us/step - loss: 91.8144 - val_loss: 103.1861\n",
      "\n",
      "Epoch 00010: val_loss improved from 107.72899 to 103.18609, saving model to ./model\n",
      "Epoch 11/1000\n",
      "4396/4396 [==============================] - 0s 54us/step - loss: 91.5619 - val_loss: 101.9018\n",
      "\n",
      "Epoch 00011: val_loss improved from 103.18609 to 101.90177, saving model to ./model\n",
      "Epoch 12/1000\n",
      "4396/4396 [==============================] - 0s 53us/step - loss: 90.5382 - val_loss: 100.6806\n",
      "\n",
      "Epoch 00012: val_loss improved from 101.90177 to 100.68064, saving model to ./model\n",
      "Epoch 13/1000\n",
      "4396/4396 [==============================] - 0s 53us/step - loss: 90.1837 - val_loss: 98.7355\n",
      "\n",
      "Epoch 00013: val_loss improved from 100.68064 to 98.73555, saving model to ./model\n",
      "Epoch 14/1000\n",
      "4396/4396 [==============================] - 0s 53us/step - loss: 89.8795 - val_loss: 97.2055\n",
      "\n",
      "Epoch 00014: val_loss improved from 98.73555 to 97.20546, saving model to ./model\n",
      "Epoch 15/1000\n",
      "4396/4396 [==============================] - 0s 81us/step - loss: 89.0163 - val_loss: 97.4291\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 97.20546\n",
      "Epoch 16/1000\n",
      "4396/4396 [==============================] - 0s 89us/step - loss: 88.4319 - val_loss: 102.3307\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 97.20546\n",
      "Epoch 17/1000\n",
      "4396/4396 [==============================] - 0s 108us/step - loss: 88.5009 - val_loss: 103.0567\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 97.20546\n",
      "Epoch 18/1000\n",
      "4396/4396 [==============================] - 0s 86us/step - loss: 87.6409 - val_loss: 94.0511\n",
      "\n",
      "Epoch 00018: val_loss improved from 97.20546 to 94.05106, saving model to ./model\n",
      "Epoch 19/1000\n",
      "4396/4396 [==============================] - 0s 74us/step - loss: 87.1593 - val_loss: 91.4056\n",
      "\n",
      "Epoch 00019: val_loss improved from 94.05106 to 91.40556, saving model to ./model\n",
      "Epoch 20/1000\n",
      "4396/4396 [==============================] - 0s 69us/step - loss: 86.8139 - val_loss: 92.0226\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 91.40556\n",
      "Epoch 21/1000\n",
      "4396/4396 [==============================] - 0s 52us/step - loss: 86.5426 - val_loss: 89.7588\n",
      "\n",
      "Epoch 00021: val_loss improved from 91.40556 to 89.75881, saving model to ./model\n",
      "Epoch 22/1000\n",
      "4396/4396 [==============================] - 0s 53us/step - loss: 85.8882 - val_loss: 87.9010\n",
      "\n",
      "Epoch 00022: val_loss improved from 89.75881 to 87.90097, saving model to ./model\n",
      "Epoch 23/1000\n",
      "4396/4396 [==============================] - 0s 58us/step - loss: 85.3072 - val_loss: 89.8419\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 87.90097\n",
      "Epoch 24/1000\n",
      "4396/4396 [==============================] - 0s 66us/step - loss: 85.2301 - val_loss: 85.1309\n",
      "\n",
      "Epoch 00024: val_loss improved from 87.90097 to 85.13088, saving model to ./model\n",
      "Epoch 25/1000\n",
      "4396/4396 [==============================] - 0s 79us/step - loss: 84.6776 - val_loss: 80.6464\n",
      "\n",
      "Epoch 00025: val_loss improved from 85.13088 to 80.64643, saving model to ./model\n",
      "Epoch 26/1000\n",
      "4396/4396 [==============================] - 0s 90us/step - loss: 84.1973 - val_loss: 83.7769\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 80.64643\n",
      "Epoch 27/1000\n",
      "4396/4396 [==============================] - 1s 114us/step - loss: 83.9484 - val_loss: 85.0643\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 80.64643\n",
      "Epoch 28/1000\n",
      "4396/4396 [==============================] - 1s 114us/step - loss: 82.8693 - val_loss: 78.0096\n",
      "\n",
      "Epoch 00028: val_loss improved from 80.64643 to 78.00956, saving model to ./model\n",
      "Epoch 29/1000\n",
      "4396/4396 [==============================] - 0s 66us/step - loss: 83.2372 - val_loss: 84.7905\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 78.00956\n",
      "Epoch 30/1000\n",
      "4396/4396 [==============================] - 0s 53us/step - loss: 82.3267 - val_loss: 78.6312\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 78.00956\n",
      "Epoch 31/1000\n",
      "4396/4396 [==============================] - 0s 52us/step - loss: 81.7081 - val_loss: 82.5768\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 78.00956\n",
      "Epoch 32/1000\n",
      "4396/4396 [==============================] - 0s 53us/step - loss: 81.6255 - val_loss: 78.1364\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 78.00956\n",
      "Epoch 33/1000\n",
      "4396/4396 [==============================] - 0s 65us/step - loss: 80.9181 - val_loss: 74.6666\n",
      "\n",
      "Epoch 00033: val_loss improved from 78.00956 to 74.66656, saving model to ./model\n",
      "Epoch 34/1000\n",
      "4396/4396 [==============================] - 0s 96us/step - loss: 80.7964 - val_loss: 79.3428\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 74.66656\n",
      "Epoch 35/1000\n",
      "4396/4396 [==============================] - 0s 88us/step - loss: 80.4493 - val_loss: 79.1136\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 74.66656\n",
      "Epoch 36/1000\n",
      "4396/4396 [==============================] - 0s 113us/step - loss: 79.6781 - val_loss: 74.0693\n",
      "\n",
      "Epoch 00036: val_loss improved from 74.66656 to 74.06931, saving model to ./model\n",
      "Epoch 37/1000\n",
      "4396/4396 [==============================] - 0s 84us/step - loss: 79.7648 - val_loss: 73.9736\n",
      "\n",
      "Epoch 00037: val_loss improved from 74.06931 to 73.97364, saving model to ./model\n",
      "Epoch 38/1000\n",
      "4396/4396 [==============================] - 0s 93us/step - loss: 79.0081 - val_loss: 73.2952\n",
      "\n",
      "Epoch 00038: val_loss improved from 73.97364 to 73.29518, saving model to ./model\n",
      "Epoch 39/1000\n",
      "4396/4396 [==============================] - 0s 73us/step - loss: 78.5848 - val_loss: 74.1617\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 73.29518\n",
      "Epoch 40/1000\n",
      "4396/4396 [==============================] - 0s 63us/step - loss: 78.3798 - val_loss: 72.3220\n",
      "\n",
      "Epoch 00040: val_loss improved from 73.29518 to 72.32202, saving model to ./model\n",
      "Epoch 41/1000\n",
      "4396/4396 [==============================] - 0s 64us/step - loss: 77.6940 - val_loss: 70.8539\n",
      "\n",
      "Epoch 00041: val_loss improved from 72.32202 to 70.85394, saving model to ./model\n",
      "Epoch 42/1000\n",
      "4396/4396 [==============================] - 0s 63us/step - loss: 77.8868 - val_loss: 70.9184\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 70.85394\n",
      "Epoch 43/1000\n",
      "4396/4396 [==============================] - 0s 83us/step - loss: 77.4105 - val_loss: 71.3987\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 70.85394\n",
      "Epoch 44/1000\n",
      "4396/4396 [==============================] - 0s 83us/step - loss: 77.4865 - val_loss: 73.4338\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 70.85394\n",
      "Epoch 45/1000\n",
      "4396/4396 [==============================] - 0s 80us/step - loss: 76.8132 - val_loss: 71.3905\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 70.85394\n",
      "Epoch 46/1000\n",
      "4396/4396 [==============================] - 0s 72us/step - loss: 76.3710 - val_loss: 71.0728\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 70.85394\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4396/4396 [==============================] - 0s 81us/step - loss: 76.1757 - val_loss: 73.5447\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 70.85394\n",
      "Epoch 48/1000\n",
      "4396/4396 [==============================] - 0s 78us/step - loss: 75.7315 - val_loss: 71.1553\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 70.85394\n",
      "Epoch 49/1000\n",
      "4396/4396 [==============================] - 0s 55us/step - loss: 75.3709 - val_loss: 78.5295\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 70.85394\n",
      "Epoch 50/1000\n",
      "4396/4396 [==============================] - 0s 47us/step - loss: 75.4412 - val_loss: 72.4687\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 70.85394\n",
      "Epoch 51/1000\n",
      "4396/4396 [==============================] - 0s 60us/step - loss: 74.9920 - val_loss: 70.4879\n",
      "\n",
      "Epoch 00051: val_loss improved from 70.85394 to 70.48794, saving model to ./model\n",
      "Epoch 52/1000\n",
      "4396/4396 [==============================] - 0s 63us/step - loss: 75.5144 - val_loss: 71.7461\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 70.48794\n",
      "Epoch 53/1000\n",
      "4396/4396 [==============================] - 0s 95us/step - loss: 74.9578 - val_loss: 71.5578\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 70.48794\n",
      "Epoch 54/1000\n",
      "4396/4396 [==============================] - 0s 70us/step - loss: 74.5162 - val_loss: 75.2022\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 70.48794\n",
      "Epoch 55/1000\n",
      "4396/4396 [==============================] - 1s 115us/step - loss: 74.8024 - val_loss: 70.7198\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 70.48794\n",
      "Epoch 56/1000\n",
      "4396/4396 [==============================] - 0s 83us/step - loss: 74.4308 - val_loss: 73.1491\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 70.48794\n",
      "Epoch 57/1000\n",
      "4396/4396 [==============================] - 0s 113us/step - loss: 73.9251 - val_loss: 69.7312\n",
      "\n",
      "Epoch 00057: val_loss improved from 70.48794 to 69.73121, saving model to ./model\n",
      "Epoch 58/1000\n",
      "4396/4396 [==============================] - 0s 88us/step - loss: 73.5372 - val_loss: 71.4385\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 69.73121\n",
      "Epoch 59/1000\n",
      "4396/4396 [==============================] - 0s 52us/step - loss: 72.8788 - val_loss: 75.2094\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 69.73121\n",
      "Epoch 60/1000\n",
      "4396/4396 [==============================] - 0s 61us/step - loss: 72.7393 - val_loss: 77.0836\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 69.73121\n",
      "Epoch 61/1000\n",
      "4396/4396 [==============================] - 0s 71us/step - loss: 73.0306 - val_loss: 73.2433\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 69.73121\n",
      "Epoch 62/1000\n",
      "4396/4396 [==============================] - 0s 90us/step - loss: 72.0705 - val_loss: 72.4961\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 69.73121\n",
      "Epoch 63/1000\n",
      "4396/4396 [==============================] - 0s 79us/step - loss: 72.5458 - val_loss: 70.6880\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 69.73121\n",
      "Epoch 64/1000\n",
      "4396/4396 [==============================] - 0s 94us/step - loss: 71.9668 - val_loss: 72.5144\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 69.73121\n",
      "Epoch 65/1000\n",
      "4396/4396 [==============================] - 0s 82us/step - loss: 71.4319 - val_loss: 69.7096\n",
      "\n",
      "Epoch 00065: val_loss improved from 69.73121 to 69.70962, saving model to ./model\n",
      "Epoch 66/1000\n",
      "4396/4396 [==============================] - 0s 75us/step - loss: 71.6092 - val_loss: 71.1245\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 69.70962\n",
      "Epoch 67/1000\n",
      "4396/4396 [==============================] - 0s 68us/step - loss: 71.3470 - val_loss: 73.3097\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 69.70962\n",
      "Epoch 68/1000\n",
      "4396/4396 [==============================] - 0s 73us/step - loss: 71.7646 - val_loss: 72.5872\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 69.70962\n",
      "Epoch 69/1000\n",
      "4396/4396 [==============================] - 0s 59us/step - loss: 71.0253 - val_loss: 68.3326\n",
      "\n",
      "Epoch 00069: val_loss improved from 69.70962 to 68.33256, saving model to ./model\n",
      "Epoch 70/1000\n",
      "4396/4396 [==============================] - 0s 56us/step - loss: 70.9530 - val_loss: 74.2086\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 68.33256\n",
      "Epoch 71/1000\n",
      "4396/4396 [==============================] - 0s 55us/step - loss: 70.4061 - val_loss: 69.8596\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 68.33256\n",
      "Epoch 72/1000\n",
      "4396/4396 [==============================] - 0s 70us/step - loss: 70.6136 - val_loss: 76.1638\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 68.33256\n",
      "Epoch 73/1000\n",
      "4396/4396 [==============================] - 0s 76us/step - loss: 70.3802 - val_loss: 74.4044\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 68.33256\n",
      "Epoch 74/1000\n",
      "4396/4396 [==============================] - 0s 86us/step - loss: 70.4140 - val_loss: 72.3372\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 68.33256\n",
      "Epoch 75/1000\n",
      "4396/4396 [==============================] - 0s 76us/step - loss: 69.8144 - val_loss: 67.6362\n",
      "\n",
      "Epoch 00075: val_loss improved from 68.33256 to 67.63621, saving model to ./model\n",
      "Epoch 76/1000\n",
      "4396/4396 [==============================] - 0s 71us/step - loss: 69.7265 - val_loss: 64.2120\n",
      "\n",
      "Epoch 00076: val_loss improved from 67.63621 to 64.21196, saving model to ./model\n",
      "Epoch 77/1000\n",
      "4396/4396 [==============================] - 0s 67us/step - loss: 69.4363 - val_loss: 69.8107\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 64.21196\n",
      "Epoch 78/1000\n",
      "4396/4396 [==============================] - 0s 60us/step - loss: 70.1882 - val_loss: 72.3860\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 64.21196\n",
      "Epoch 79/1000\n",
      "4396/4396 [==============================] - 0s 57us/step - loss: 70.0665 - val_loss: 64.7442\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 64.21196\n",
      "Epoch 80/1000\n",
      "4396/4396 [==============================] - 0s 55us/step - loss: 68.5700 - val_loss: 69.3881\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 64.21196\n",
      "Epoch 81/1000\n",
      "4396/4396 [==============================] - 1s 135us/step - loss: 68.9899 - val_loss: 64.1452\n",
      "\n",
      "Epoch 00081: val_loss improved from 64.21196 to 64.14518, saving model to ./model\n",
      "Epoch 82/1000\n",
      "4396/4396 [==============================] - 0s 108us/step - loss: 69.3439 - val_loss: 66.3045\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 64.14518\n",
      "Epoch 83/1000\n",
      "4396/4396 [==============================] - 0s 74us/step - loss: 68.6588 - val_loss: 66.2691\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 64.14518\n",
      "Epoch 84/1000\n",
      "4396/4396 [==============================] - 0s 86us/step - loss: 68.3829 - val_loss: 75.2185\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 64.14518\n",
      "Epoch 85/1000\n",
      "4396/4396 [==============================] - 0s 80us/step - loss: 68.3454 - val_loss: 63.0444\n",
      "\n",
      "Epoch 00085: val_loss improved from 64.14518 to 63.04437, saving model to ./model\n",
      "Epoch 86/1000\n",
      "4396/4396 [==============================] - 0s 105us/step - loss: 67.5231 - val_loss: 65.7567\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 63.04437\n",
      "Epoch 87/1000\n",
      "4396/4396 [==============================] - 0s 104us/step - loss: 68.6630 - val_loss: 62.6395\n",
      "\n",
      "Epoch 00087: val_loss improved from 63.04437 to 62.63948, saving model to ./model\n",
      "Epoch 88/1000\n",
      "4396/4396 [==============================] - 0s 67us/step - loss: 67.9715 - val_loss: 64.1790\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 62.63948\n",
      "Epoch 89/1000\n",
      "4396/4396 [==============================] - 0s 69us/step - loss: 67.8069 - val_loss: 61.3252\n",
      "\n",
      "Epoch 00089: val_loss improved from 62.63948 to 61.32524, saving model to ./model\n",
      "Epoch 90/1000\n",
      "4396/4396 [==============================] - 0s 80us/step - loss: 67.4251 - val_loss: 62.7089\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 61.32524\n",
      "Epoch 91/1000\n",
      "4396/4396 [==============================] - 0s 77us/step - loss: 67.0945 - val_loss: 63.2924\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 61.32524\n",
      "Epoch 92/1000\n",
      "4396/4396 [==============================] - 0s 84us/step - loss: 66.6005 - val_loss: 66.9665\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 61.32524\n",
      "Epoch 93/1000\n",
      "4396/4396 [==============================] - 0s 102us/step - loss: 67.1749 - val_loss: 65.5361\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 61.32524\n",
      "Epoch 94/1000\n",
      "4396/4396 [==============================] - 0s 77us/step - loss: 66.9373 - val_loss: 71.1530\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 61.32524\n",
      "Epoch 95/1000\n",
      "4396/4396 [==============================] - 0s 97us/step - loss: 66.5586 - val_loss: 68.0659\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 61.32524\n",
      "Epoch 96/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4396/4396 [==============================] - ETA: 0s - loss: 68.49 - 0s 104us/step - loss: 66.9741 - val_loss: 66.0861\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 61.32524\n",
      "Epoch 97/1000\n",
      "4396/4396 [==============================] - 0s 69us/step - loss: 66.2997 - val_loss: 62.1635\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 61.32524\n",
      "Epoch 98/1000\n",
      "4396/4396 [==============================] - 0s 51us/step - loss: 66.2409 - val_loss: 65.8283\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 61.32524\n",
      "Epoch 99/1000\n",
      "4396/4396 [==============================] - 0s 63us/step - loss: 65.1691 - val_loss: 77.8326\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 61.32524\n",
      "Epoch 100/1000\n",
      "4396/4396 [==============================] - 0s 72us/step - loss: 66.6906 - val_loss: 64.1510\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 61.32524\n",
      "Epoch 101/1000\n",
      "4396/4396 [==============================] - 0s 99us/step - loss: 65.9997 - val_loss: 68.8219\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 61.32524\n",
      "Epoch 102/1000\n",
      "4396/4396 [==============================] - 0s 76us/step - loss: 66.2887 - val_loss: 66.6921\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 61.32524\n",
      "Epoch 103/1000\n",
      "4396/4396 [==============================] - 0s 88us/step - loss: 65.7992 - val_loss: 67.8456\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 61.32524\n",
      "Epoch 104/1000\n",
      "4396/4396 [==============================] - 0s 101us/step - loss: 65.3143 - val_loss: 74.6483\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 61.32524\n",
      "Epoch 105/1000\n",
      "4396/4396 [==============================] - 0s 76us/step - loss: 65.2020 - val_loss: 71.9167\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 61.32524\n",
      "Epoch 106/1000\n",
      "4396/4396 [==============================] - 0s 85us/step - loss: 64.9546 - val_loss: 67.8301\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 61.32524\n",
      "Epoch 107/1000\n",
      "4396/4396 [==============================] - 0s 60us/step - loss: 64.6551 - val_loss: 72.6112\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 61.32524\n",
      "Epoch 108/1000\n",
      "4396/4396 [==============================] - 0s 73us/step - loss: 65.1874 - val_loss: 73.3111\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 61.32524\n",
      "Epoch 109/1000\n",
      "4396/4396 [==============================] - 0s 53us/step - loss: 64.5583 - val_loss: 76.5770\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 61.32524\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_, y_train, validation_data=(X_val_, y_val), callbacks=calbacks,  epochs=1000, verbose=1)\n",
    "model.load_weights('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8.,  6.,  4.,  5.,  3.,  4.,  1., 11.,  6.,  4.,  6.,  7.,  7.,\n",
       "         2.,  4., 10.,  1.,  1.,  1.,  1.]), array([[15.520767],\n",
       "        [ 9.653181],\n",
       "        [10.037788],\n",
       "        [16.045458],\n",
       "        [10.036268],\n",
       "        [10.490374],\n",
       "        [11.125574],\n",
       "        [16.413357],\n",
       "        [10.955526],\n",
       "        [10.951387],\n",
       "        [17.089039],\n",
       "        [10.256915],\n",
       "        [15.738028],\n",
       "        [13.996916],\n",
       "        [10.201628],\n",
       "        [14.93507 ],\n",
       "        [12.221077],\n",
       "        [12.80347 ],\n",
       "        [12.859479],\n",
       "        [10.357521]], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lin = model.predict(X_val_)\n",
    "np.array(y_val[100:120]), y_lin[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(criterion='mse',n_estimators=125, min_samples_leaf=2, random_state=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=2, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=125, n_jobs=-1,\n",
       "           oob_score=False, random_state=3, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ExtraTreesRegressor(criterion='mse',n_estimators=125, max_depth=14, min_samples_leaf=5, random_state=3, n_jobs=-1)\n",
    "ext = ExtraTreesRegressor(criterion='mse',n_estimators=200, max_depth=15, random_state=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=15,\n",
       "          max_features='auto', max_leaf_nodes=None,\n",
       "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "          min_samples_leaf=1, min_samples_split=2,\n",
       "          min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
       "          oob_score=False, random_state=3, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rf = rf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ext = ext.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANO': 0.041117012566097816,\n",
       " 'MES': 0.3224793475543148,\n",
       " 'DESCRICAO': 0.1344313463432912,\n",
       " 'NOME': 0.08659593044035953,\n",
       " 'TAMANHO': 0.11608902876893579,\n",
       " 'last_year': 0.29928733432700066}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(X_train.columns, rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANO': 0.04168956371722239,\n",
       " 'MES': 0.33467144628405876,\n",
       " 'DESCRICAO': 0.16221869194969749,\n",
       " 'NOME': 0.07247567813765399,\n",
       " 'TAMANHO': 0.10630512123409158,\n",
       " 'last_year': 0.28263949867727584}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(X_train.columns, ext.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.73674131143746"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.65846730626679"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'up' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-8595e09daf64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'up' is not defined"
     ]
    }
   ],
   "source": [
    "mean_squared_error(y_val, list(map(up, y_lin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.15746631349165"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(validacao.QUANTIDADE.values, (y_rf + y_ext)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench(u, a_):\n",
    "    try:\n",
    "        return a_[u]\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino['unique_id'] = treino.DESCRICAO.astype(str).apply(lambda s : s.zfill(2)) + treino.NOME.astype(str).apply(lambda s : s.zfill(2)) + treino.TAMANHO.astype(str).apply(lambda s : s.zfill(2)) + treino.MES.astype(str).apply(lambda s : s.zfill(2))\n",
    "b1 = treino.groupby(['MES', 'DESCRICAO', 'NOME', 'TAMANHO', 'unique_id']).mean().reset_index()\n",
    "b2 = treino[treino.ANO == 2017].groupby(['MES', 'DESCRICAO', 'NOME', 'TAMANHO', 'unique_id']).mean().reset_index()\n",
    "b_1 = dict(zip(b1.unique_id, b1.QUANTIDADE))\n",
    "b_2 = dict(zip(b2.unique_id, b2.QUANTIDADE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao['unique_id'] = validacao.DESCRICAO.astype(str).apply(lambda s : s.zfill(2)) + validacao.NOME.astype(str).apply(lambda s : s.zfill(2)) + validacao.TAMANHO.astype(str).apply(lambda s : s.zfill(2)) + validacao.MES.astype(str).apply(lambda s : s.zfill(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unana'] = df.groupby(['DESCRICAO', 'NOME', 'TAMANHO'])['QUANTIDADE'].shift(12).fillna(-1) #+ df.groupby(['DESCRICAO', 'NOME'])['QUANTIDADE'].shift(24).fillna(-1)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANO</th>\n",
       "      <th>MES</th>\n",
       "      <th>DESCRICAO</th>\n",
       "      <th>NOME</th>\n",
       "      <th>TAMANHO</th>\n",
       "      <th>DATE_BLOCK</th>\n",
       "      <th>QUANTIDADE</th>\n",
       "      <th>last_year</th>\n",
       "      <th>unana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>ALDEIA DA SERRA</td>\n",
       "      <td>02</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4402</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>02</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>04</td>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>ALDEIA DA SERRA</td>\n",
       "      <td>04</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4403</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>04</td>\n",
       "      <td>24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>06</td>\n",
       "      <td>24</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4401</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>ALDEIA DA SERRA</td>\n",
       "      <td>06</td>\n",
       "      <td>24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4404</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>06</td>\n",
       "      <td>24</td>\n",
       "      <td>20.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>08</td>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA COTTON</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>08</td>\n",
       "      <td>24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4410</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>ALDEIA DA SERRA</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4411</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>ITAIM</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>16.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4415</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>G</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4412</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>ITAIM</td>\n",
       "      <td>M</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4416</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>M</td>\n",
       "      <td>24</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4408</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>P</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>P</td>\n",
       "      <td>24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>PP</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4418</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA CICLISTA SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>PP</td>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA DE MOLETOM</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>00</td>\n",
       "      <td>24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4429</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA DE MOLETOM</td>\n",
       "      <td>ALDEIA DA SERRA</td>\n",
       "      <td>00</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA DE MOLETOM</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>00</td>\n",
       "      <td>24</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA DE MOLETOM</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>02</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA DE MOLETOM</td>\n",
       "      <td>ALDEIA DA SERRA</td>\n",
       "      <td>02</td>\n",
       "      <td>24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4441</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA DE MOLETOM</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>02</td>\n",
       "      <td>24</td>\n",
       "      <td>27.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BERMUDA DE MOLETOM</td>\n",
       "      <td>ACLIMAÇÃO</td>\n",
       "      <td>04</td>\n",
       "      <td>24</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA CURTA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>36.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA CURTA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>G</td>\n",
       "      <td>28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA CURTA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5354</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA CURTA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>P</td>\n",
       "      <td>28</td>\n",
       "      <td>24.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA CURTA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>PP</td>\n",
       "      <td>28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5356</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA LONGA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>00</td>\n",
       "      <td>28</td>\n",
       "      <td>8.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA LONGA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>02</td>\n",
       "      <td>28</td>\n",
       "      <td>9.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5358</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA LONGA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>04</td>\n",
       "      <td>28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5359</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA LONGA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>06</td>\n",
       "      <td>28</td>\n",
       "      <td>11.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA LONGA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>08</td>\n",
       "      <td>28</td>\n",
       "      <td>28.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5361</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA LONGA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>8.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5362</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CAMISETA MANGA LONGA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CORSARIO COTTON</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>02</td>\n",
       "      <td>28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CORSARIO COTTON</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>04</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5365</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CORSARIO COTTON</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>06</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5366</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CORSARIO COTTON</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>08</td>\n",
       "      <td>28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CORSARIO SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5368</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CORSARIO SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>7.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CORSARIO SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5370</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CORSARIO SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>P</td>\n",
       "      <td>28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CORSARIO SUPLEX</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>PP</td>\n",
       "      <td>28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5372</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>JALECO</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>JALECO</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>P</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5374</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>REGATA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>06</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>REGATA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5376</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>02</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>04</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>06</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5379</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>08</td>\n",
       "      <td>28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SHORT SAIA</td>\n",
       "      <td>VERBO DIVINO</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>985 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ANO  MES                DESCRICAO             NOME TAMANHO  \\\n",
       "4399  2018.0  1.0  BERMUDA CICLISTA COTTON  ALDEIA DA SERRA      02   \n",
       "4402  2018.0  1.0  BERMUDA CICLISTA COTTON     VERBO DIVINO      02   \n",
       "4396  2018.0  1.0  BERMUDA CICLISTA COTTON        ACLIMAÇÃO      04   \n",
       "4400  2018.0  1.0  BERMUDA CICLISTA COTTON  ALDEIA DA SERRA      04   \n",
       "4403  2018.0  1.0  BERMUDA CICLISTA COTTON     VERBO DIVINO      04   \n",
       "4397  2018.0  1.0  BERMUDA CICLISTA COTTON        ACLIMAÇÃO      06   \n",
       "4401  2018.0  1.0  BERMUDA CICLISTA COTTON  ALDEIA DA SERRA      06   \n",
       "4404  2018.0  1.0  BERMUDA CICLISTA COTTON     VERBO DIVINO      06   \n",
       "4398  2018.0  1.0  BERMUDA CICLISTA COTTON        ACLIMAÇÃO      08   \n",
       "4405  2018.0  1.0  BERMUDA CICLISTA COTTON     VERBO DIVINO      08   \n",
       "4406  2018.0  1.0  BERMUDA CICLISTA SUPLEX        ACLIMAÇÃO      10   \n",
       "4413  2018.0  1.0  BERMUDA CICLISTA SUPLEX     VERBO DIVINO      10   \n",
       "4407  2018.0  1.0  BERMUDA CICLISTA SUPLEX        ACLIMAÇÃO      12   \n",
       "4410  2018.0  1.0  BERMUDA CICLISTA SUPLEX  ALDEIA DA SERRA      12   \n",
       "4411  2018.0  1.0  BERMUDA CICLISTA SUPLEX            ITAIM      12   \n",
       "4414  2018.0  1.0  BERMUDA CICLISTA SUPLEX     VERBO DIVINO      12   \n",
       "4415  2018.0  1.0  BERMUDA CICLISTA SUPLEX     VERBO DIVINO       G   \n",
       "4412  2018.0  1.0  BERMUDA CICLISTA SUPLEX            ITAIM       M   \n",
       "4416  2018.0  1.0  BERMUDA CICLISTA SUPLEX     VERBO DIVINO       M   \n",
       "4408  2018.0  1.0  BERMUDA CICLISTA SUPLEX        ACLIMAÇÃO       P   \n",
       "4417  2018.0  1.0  BERMUDA CICLISTA SUPLEX     VERBO DIVINO       P   \n",
       "4409  2018.0  1.0  BERMUDA CICLISTA SUPLEX        ACLIMAÇÃO      PP   \n",
       "4418  2018.0  1.0  BERMUDA CICLISTA SUPLEX     VERBO DIVINO      PP   \n",
       "4419  2018.0  1.0       BERMUDA DE MOLETOM        ACLIMAÇÃO      00   \n",
       "4429  2018.0  1.0       BERMUDA DE MOLETOM  ALDEIA DA SERRA      00   \n",
       "4440  2018.0  1.0       BERMUDA DE MOLETOM     VERBO DIVINO      00   \n",
       "4420  2018.0  1.0       BERMUDA DE MOLETOM        ACLIMAÇÃO      02   \n",
       "4430  2018.0  1.0       BERMUDA DE MOLETOM  ALDEIA DA SERRA      02   \n",
       "4441  2018.0  1.0       BERMUDA DE MOLETOM     VERBO DIVINO      02   \n",
       "4421  2018.0  1.0       BERMUDA DE MOLETOM        ACLIMAÇÃO      04   \n",
       "...      ...  ...                      ...              ...     ...   \n",
       "5351  2018.0  5.0     CAMISETA MANGA CURTA     VERBO DIVINO      12   \n",
       "5352  2018.0  5.0     CAMISETA MANGA CURTA     VERBO DIVINO       G   \n",
       "5353  2018.0  5.0     CAMISETA MANGA CURTA     VERBO DIVINO       M   \n",
       "5354  2018.0  5.0     CAMISETA MANGA CURTA     VERBO DIVINO       P   \n",
       "5355  2018.0  5.0     CAMISETA MANGA CURTA     VERBO DIVINO      PP   \n",
       "5356  2018.0  5.0     CAMISETA MANGA LONGA     VERBO DIVINO      00   \n",
       "5357  2018.0  5.0     CAMISETA MANGA LONGA     VERBO DIVINO      02   \n",
       "5358  2018.0  5.0     CAMISETA MANGA LONGA     VERBO DIVINO      04   \n",
       "5359  2018.0  5.0     CAMISETA MANGA LONGA     VERBO DIVINO      06   \n",
       "5360  2018.0  5.0     CAMISETA MANGA LONGA     VERBO DIVINO      08   \n",
       "5361  2018.0  5.0     CAMISETA MANGA LONGA     VERBO DIVINO      10   \n",
       "5362  2018.0  5.0     CAMISETA MANGA LONGA     VERBO DIVINO      12   \n",
       "5363  2018.0  5.0          CORSARIO COTTON     VERBO DIVINO      02   \n",
       "5364  2018.0  5.0          CORSARIO COTTON     VERBO DIVINO      04   \n",
       "5365  2018.0  5.0          CORSARIO COTTON     VERBO DIVINO      06   \n",
       "5366  2018.0  5.0          CORSARIO COTTON     VERBO DIVINO      08   \n",
       "5367  2018.0  5.0          CORSARIO SUPLEX     VERBO DIVINO      10   \n",
       "5368  2018.0  5.0          CORSARIO SUPLEX     VERBO DIVINO      12   \n",
       "5369  2018.0  5.0          CORSARIO SUPLEX     VERBO DIVINO       M   \n",
       "5370  2018.0  5.0          CORSARIO SUPLEX     VERBO DIVINO       P   \n",
       "5371  2018.0  5.0          CORSARIO SUPLEX     VERBO DIVINO      PP   \n",
       "5372  2018.0  5.0                   JALECO     VERBO DIVINO      14   \n",
       "5373  2018.0  5.0                   JALECO     VERBO DIVINO       P   \n",
       "5374  2018.0  5.0                   REGATA     VERBO DIVINO      06   \n",
       "5375  2018.0  5.0                   REGATA     VERBO DIVINO      12   \n",
       "5376  2018.0  5.0               SHORT SAIA     VERBO DIVINO      02   \n",
       "5377  2018.0  5.0               SHORT SAIA     VERBO DIVINO      04   \n",
       "5378  2018.0  5.0               SHORT SAIA     VERBO DIVINO      06   \n",
       "5379  2018.0  5.0               SHORT SAIA     VERBO DIVINO      08   \n",
       "5380  2018.0  5.0               SHORT SAIA     VERBO DIVINO      10   \n",
       "\n",
       "      DATE_BLOCK  QUANTIDADE  last_year  unana  \n",
       "4399          24         1.0        2.0   -1.0  \n",
       "4402          24         3.0        5.0   -1.0  \n",
       "4396          24         8.0        9.0   -1.0  \n",
       "4400          24         1.0        6.0   -1.0  \n",
       "4403          24        13.0       33.0   -1.0  \n",
       "4397          24        12.0       14.0   -1.0  \n",
       "4401          24         2.0        1.0   -1.0  \n",
       "4404          24        20.0       27.0   -1.0  \n",
       "4398          24         8.0        4.0   -1.0  \n",
       "4405          24        13.0       34.0    6.0  \n",
       "4406          24        12.0       11.0   -1.0  \n",
       "4413          24         8.0       52.0   11.0  \n",
       "4407          24         5.0        4.0   -1.0  \n",
       "4410          24         6.0       -1.0   -1.0  \n",
       "4411          24         2.0        8.0   -1.0  \n",
       "4414          24        16.0       38.0    3.0  \n",
       "4415          24         1.0       -1.0   -1.0  \n",
       "4412          24         1.0        4.0   -1.0  \n",
       "4416          24         4.0        7.0   -1.0  \n",
       "4408          24         1.0       -1.0   -1.0  \n",
       "4417          24         2.0       10.0   -1.0  \n",
       "4409          24         3.0        3.0   -1.0  \n",
       "4418          24         8.0       24.0   -1.0  \n",
       "4419          24         5.0        8.0   -1.0  \n",
       "4429          24         3.0        1.0   -1.0  \n",
       "4440          24        11.0       19.0   22.0  \n",
       "4420          24         3.0       13.0   -1.0  \n",
       "4430          24         2.0        8.0   -1.0  \n",
       "4441          24        27.0       42.0   20.0  \n",
       "4421          24         9.0       23.0   30.0  \n",
       "...          ...         ...        ...    ...  \n",
       "5351          28        36.0      313.0   16.0  \n",
       "5352          28         4.0       50.0    4.0  \n",
       "5353          28         2.0      123.0   17.0  \n",
       "5354          28        24.0      101.0   12.0  \n",
       "5355          28        16.0      211.0    9.0  \n",
       "5356          28         8.0       39.0    2.0  \n",
       "5357          28         9.0       55.0   21.0  \n",
       "5358          28        16.0       81.0    9.0  \n",
       "5359          28        11.0       96.0   12.0  \n",
       "5360          28        28.0       62.0   10.0  \n",
       "5361          28         8.0       53.0   10.0  \n",
       "5362          28        10.0       40.0    5.0  \n",
       "5363          28         4.0       13.0    3.0  \n",
       "5364          28         3.0       42.0    2.0  \n",
       "5365          28         3.0       40.0    1.0  \n",
       "5366          28         4.0       14.0    2.0  \n",
       "5367          28         3.0       50.0    9.0  \n",
       "5368          28         7.0       54.0    6.0  \n",
       "5369          28         2.0       12.0    9.0  \n",
       "5370          28         4.0       16.0    2.0  \n",
       "5371          28         4.0       29.0    1.0  \n",
       "5372          28         1.0       11.0   -1.0  \n",
       "5373          28         1.0        9.0   -1.0  \n",
       "5374          28         1.0       19.0   -1.0  \n",
       "5375          28         3.0        6.0   -1.0  \n",
       "5376          28         1.0       26.0    3.0  \n",
       "5377          28         1.0       57.0   12.0  \n",
       "5378          28         3.0       58.0    2.0  \n",
       "5379          28         5.0       52.0    8.0  \n",
       "5380          28         2.0       36.0    3.0  \n",
       "\n",
       "[985 rows x 9 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.ANO == 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_sazonal = validacao.unique_id.apply(lambda u: bench(u, b_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultimo_ano = validacao.unique_id.apply(lambda u: bench(u, b_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281.4020304568528"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(validacao.QUANTIDADE.values, df[df.ANO == 2018]['unana'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.12791878172588"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(validacao.QUANTIDADE.values, ultimo_ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X Real</th>\n",
       "      <th>Y RF</th>\n",
       "      <th>Z Media sazonal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5376</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5379</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      X Real  Y RF  Z Media sazonal\n",
       "5376     1.0   3.0              1.0\n",
       "5377     1.0   3.0              3.5\n",
       "5378     3.0   3.0              2.0\n",
       "5379     5.0   3.0              1.0\n",
       "5380     2.0   3.0              1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'X Real' : validacao.QUANTIDADE,\n",
    "    'Y RF' : np.round((y_rf + y_ext)/2),\n",
    "    'Z Media sazonal': media_sazonal\n",
    "}).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao = df[df.ANO >= 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao['Y RF'] = (y_rf + y_ext)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao['Z Media sazonal'] = media_sazonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
